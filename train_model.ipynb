{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DEVICE = 'cuda'\n",
    "LR = 1e-5\n",
    "NUM_ITER = 30\n",
    "\n",
    "out_dir_base = Path('data/exp_access/reliance_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_exp_results\n",
    "valid_user, task_result, f_score, _, _ = load_exp_results.load()\n",
    "\n",
    "f_score = f_score[f_score['condition'].map(lambda x: 'RandomCueUser' in x)]\n",
    "task_result = task_result[task_result['user_class'].map(lambda x: 'RandomCueUser' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import inference\n",
    "inference.init()\n",
    "\n",
    "def which_dtype(array):\n",
    "    if type(array) == torch.Tensor:\n",
    "        return array.dtype\n",
    "    if np.issubdtype(array.dtype, np.integer):\n",
    "        return torch.long\n",
    "    elif np.issubdtype(array.dtype, np.floating):\n",
    "        return torch.float32\n",
    "    elif np.issubdtype(array.dtype, np.bool_):\n",
    "        return torch.bool\n",
    "\n",
    "def get_input(user_id, target_index):\n",
    "    example = task_result[task_result['user_id'] == user_id].sort_values('episode_id')\n",
    "\n",
    "    src = dict()\n",
    "    src['middles'] = np.vstack(example.apply(lambda raw: inference.get_y(raw['dataset_name'], 'captcha-09az+capital-color', raw['ground_truth'], ['middle'])[0], axis=1).values).astype(np.float32)\n",
    "    src['cues'] = example['system_action_token'].values.copy()\n",
    "    src['decisions'] = example['user_decision_token'].values.copy()\n",
    "\n",
    "    src = {key: torch.tensor(value, dtype=which_dtype(value)) for key, value in src.items()}\n",
    "    mask = {'middles': torch.zeros((src['middles'].shape[0]), dtype=torch.bool)}\n",
    "\n",
    "    mask['middles'][target_index+1:] = True\n",
    "\n",
    "    #ys = np.vstack(example.apply(lambda raw: inference.get_y(raw['dataset_name'], 'captcha-09az+capital-color', raw['ground_truth'], ['y'])[0], axis=1).values).astype(np.float32)\n",
    "    ys = example.apply(lambda raw: inference.get_y(raw['dataset_name'], 'captcha-09az+capital-color', raw['ground_truth'], ['y'])[0], axis=1).values[target_index]\n",
    "    return src, mask, ys\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_ids, rtn_acc=False):\n",
    "        self.user_ids = user_ids\n",
    "        self.n_episode = 60\n",
    "        self.rtn_acc = rtn_acc\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episode * len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx // self.n_episode]\n",
    "        episode_idx = idx % self.n_episode\n",
    "        src, mask, ys = get_input(user_id, episode_idx)\n",
    "        length = src['middles'].shape[0]\n",
    "\n",
    "        src = {key: self._padding(value, self.n_episode, 0) for key, value in src.items()}\n",
    "        mask = {key: self._padding(value, self.n_episode, True) for key, value in mask.items()}\n",
    "\n",
    "        if length <= episode_idx:\n",
    "            # ignore_index\n",
    "            label = [torch.tensor(-100)]\n",
    "        else:\n",
    "            # 0: AI, 1: SELF\n",
    "            label = src['decisions'][episode_idx].clone() - 1\n",
    "        # add [MASK] token\n",
    "        src['decisions'][episode_idx:] = MASK_TOKEN\n",
    "\n",
    "        if self.rtn_acc:\n",
    "            conf = inference.instance_confidence_calculators['captcha-09az+capital-color'].calc_score(\n",
    "                ys.reshape((1, 36, 5))\n",
    "            )\n",
    "            acc = inference.acc_estimators['captcha-09az+capital-color'].predict_proba(conf.reshape((1, 1)))[0,1]\n",
    "            return (src, mask, idx % self.n_episode), label, acc\n",
    "        return (src, mask, idx % self.n_episode), label\n",
    "\n",
    "    def _padding(self, array, length, value=None):\n",
    "        orig_length = array.shape[0]\n",
    "        if orig_length == length:\n",
    "            return array\n",
    "        elif orig_length > length:\n",
    "            raise ValueError('Input array too long.')\n",
    "        addition = torch.concat([torch.empty_like(array[:1])] * (length - array.shape[0]))\n",
    "        if value is not None:\n",
    "            addition[:] = value\n",
    "        return torch.concat([array, addition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import trust_model\n",
    "import train_model\n",
    "from const import MASK_TOKEN\n",
    "def train(out_dir_base, targets=None):\n",
    "    out_dir_base.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    ids = pd.DataFrame(sum(\n",
    "        [[{'name': f'{line[\"condition\"]}-{line[\"rate\"]}', 'index': item} for item in line[0]] \n",
    "            for _, line in pd.DataFrame(f_score.groupby(['condition', 'rate']).apply(lambda grp: grp.index)).reset_index().iterrows()],\n",
    "        list())).set_index('index')\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "    results = list()\n",
    "\n",
    "    for i_split, (idxs_train, idxs_test) in enumerate(skf.split(ids.index, ids['name'])):\n",
    "        out_dir = out_dir_base / str(i_split)\n",
    "        out_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        user_ids_train = ids.index[idxs_train]\n",
    "        user_ids_test = ids.index[idxs_test]\n",
    "        dataset_train, dataset_test = Dataset(np.hstack(user_ids_train)), Dataset(np.hstack(user_ids_test))\n",
    "        dataset_train, dataset_test = map(tuple, [dataset_train, dataset_test])\n",
    "\n",
    "        dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        if targets is None:\n",
    "            # full\n",
    "            model = trust_model.SimpleTransformerEncoder_Access(n_head=16, n_feature=128, dropout=0.2, n_hidden=2048, n_layers=3, n_out=2)\n",
    "        else:\n",
    "            # ablation\n",
    "            model = trust_model.SimpleTransformerEncoder_Access_Ablation(n_head=16, n_feature=128, dropout=0.2, n_hidden=2048, n_layers=3, n_out=2, targets=targets)\n",
    "        model = model.to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for i in range(NUM_ITER):\n",
    "            result_train = train_model.train(model, optimizer, criterion, dataloader_train, device=DEVICE)\n",
    "            result_test = train_model.test(model, criterion, dataloader_test, device=DEVICE)\n",
    "            results.append(\n",
    "                dict(i_split=i_split, i=i, train=True, **result_train)\n",
    "            )\n",
    "            results.append(\n",
    "                dict(i_split=i_split, i=i, train=False, **result_test)\n",
    "            )\n",
    "            print(i, result_train, result_test)\n",
    "            torch.save(model.state_dict(), out_dir/f'{i}.pth')\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_json(out_dir_base/'log.json')\n",
    "\n",
    "out_dir_base = Path('data/exp_access/reliance_model')\n",
    "train(out_dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "results = pd.read_json(out_dir_base/'log.json')\n",
    "\n",
    "sns.lineplot(data=results[~results['train']], x='i', y='loss')\n",
    "plt.vlines(19, 0.4, 0.65)\n",
    "plt.show()\n",
    "sns.lineplot(data=results[~results['train']], x='i', y='correct')\n",
    "plt.vlines(19, 0.6, 0.85)\n",
    "plt.show()\n",
    "sns.lineplot(data=results[results['train']], x='i', y='loss')\n",
    "plt.vlines(19, 0.4, 0.65)\n",
    "plt.show()\n",
    "results[~results['train']].groupby('i').mean(), results[~results['train']].groupby('i').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
